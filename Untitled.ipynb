{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-ms MODEL_SIZE] [-pssf SHIFT_FACTOR] [-psb]\n",
      "                             [-ppfl POST_PROC_FILT_LEN] [-lra ALPHA]\n",
      "                             [-vr VALID_RATIO] [-tr TEST_RATIO]\n",
      "                             [-bs BATCH_SIZE] [-ne NUM_EPOCHS] [-ng NGPUS]\n",
      "                             [-ld LATENT_DIM] [-eps EPOCHS_PER_SAMPLE]\n",
      "                             [-ss SAMPLE_SIZE] [-rf LMBDA] [-lr LEARNING_RATE]\n",
      "                             [-bo BETA1] [-bt BETA2] [-v]\n",
      "                             [-audio_dir AUDIO_DIR] [-output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Dell GTX\\AppData\\Roaming\\jupyter\\runtime\\kernel-0d00e75d-c82c-491f-b94c-9bb4ba7fe1de.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import json\n",
    "from utils import save_samples\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import datetime\n",
    "from wavegan import *\n",
    "from utils import *\n",
    "from logger import *\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "# =============Logger===============\n",
    "LOGGER = logging.getLogger('wavegan')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "LOGGER.info('Initialized logger.')\n",
    "init_console_logger(LOGGER)\n",
    "\n",
    "# =============Parameters===============\n",
    "args = parse_arguments()\n",
    "epochs = args['num_epochs']\n",
    "batch_size = args['batch_size']\n",
    "latent_dim = args['latent_dim']\n",
    "ngpus = args['ngpus']\n",
    "model_size = args['model_size']\n",
    "model_dir = make_path(os.path.join(args['output_dir'],\n",
    "                                   datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "args['model_dir'] = model_dir\n",
    "# save samples for every N epochs.\n",
    "epochs_per_sample = args['epochs_per_sample']\n",
    "# gradient penalty regularization factor.\n",
    "lmbda = args['lmbda']\n",
    "\n",
    "# Dir\n",
    "audio_dir = args['audio_dir']\n",
    "output_dir = args['output_dir']\n",
    "\n",
    "# =============Network===============\n",
    "netG = WaveGANGenerator(model_size=model_size, ngpus=ngpus, latent_dim=latent_dim, upsample=True)\n",
    "netD = WaveGANDiscriminator(model_size=model_size, ngpus=ngpus)\n",
    "\n",
    "if cuda:\n",
    "    netG = torch.nn.DataParallel(netG).cuda()\n",
    "    netD = torch.nn.DataParallel(netD).cuda()\n",
    "\n",
    "# \"Two time-scale update rule\"(TTUR) to update netD 4x faster than netG.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "\n",
    "# Sample noise used for generated output.\n",
    "sample_noise = torch.randn(args['sample_size'], latent_dim)\n",
    "if cuda:\n",
    "    sample_noise = sample_noise.cuda()\n",
    "sample_noise_Var = autograd.Variable(sample_noise, requires_grad=False)\n",
    "\n",
    "# Save config.\n",
    "LOGGER.info('Saving configurations...')\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(args, f)\n",
    "\n",
    "# Load data.\n",
    "LOGGER.info('Loading audio data...')\n",
    "audio_paths = get_all_audio_filepaths(audio_dir)\n",
    "train_data, valid_data, test_data, train_size = split_data(audio_paths, args['valid_ratio'],\n",
    "                                                           args['test_ratio'], batch_size)\n",
    "TOTAL_TRAIN_SAMPLES = train_size\n",
    "BATCH_NUM = TOTAL_TRAIN_SAMPLES // batch_size\n",
    "\n",
    "train_iter = iter(train_data)\n",
    "valid_iter = iter(valid_data)\n",
    "test_iter = iter(test_data)\n",
    "\n",
    "\n",
    "# =============Train===============\n",
    "history = []\n",
    "D_costs_train = []\n",
    "D_wasses_train = []\n",
    "D_costs_valid = []\n",
    "D_wasses_valid = []\n",
    "G_costs = []\n",
    "\n",
    "start = time.time()\n",
    "LOGGER.info('Starting training...EPOCHS={}, BATCH_SIZE={}, BATCH_NUM={}'.format(epochs, batch_size, BATCH_NUM))\n",
    "for epoch in range(1, epochs+1):\n",
    "    LOGGER.info(\"{} Epoch: {}/{}\".format(time_since(start), epoch, epochs))\n",
    "\n",
    "    D_cost_train_epoch = []\n",
    "    D_wass_train_epoch = []\n",
    "    D_cost_valid_epoch = []\n",
    "    D_wass_valid_epoch = []\n",
    "    G_cost_epoch = []\n",
    "    for i in range(1, BATCH_NUM+1):\n",
    "        # Set Discriminator parameters to require gradients.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        one = torch.Tensor([1]).float()\n",
    "        neg_one = one * -1\n",
    "        if cuda:\n",
    "            one = one.cuda()\n",
    "            neg_one = neg_one.cuda()\n",
    "        #############################\n",
    "        # (1) Train Discriminator\n",
    "        #############################\n",
    "        for iter_dis in range(5):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Noise\n",
    "            noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            noise_Var = Variable(noise, requires_grad=False)\n",
    "\n",
    "            real_data_Var = numpy_to_var(next(train_iter)['X'], cuda)\n",
    "\n",
    "            # a) compute loss contribution from real training data\n",
    "            D_real = netD(real_data_Var)\n",
    "            D_real = D_real.mean()  # avg loss\n",
    "            D_real.backward(neg_one)  # loss * -1\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake = autograd.Variable(netG(noise_Var).data)\n",
    "            D_fake = netD(fake)\n",
    "            D_fake = D_fake.mean()\n",
    "            D_fake.backward(one)\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_Var.data,\n",
    "                                                     fake.data, batch_size, lmbda,\n",
    "                                                     use_cuda=cuda)\n",
    "            gradient_penalty.backward(one)\n",
    "\n",
    "            # Compute cost * Wassertein loss..\n",
    "            D_cost_train = D_fake - D_real + gradient_penalty\n",
    "            D_wass_train = D_real - D_fake\n",
    "\n",
    "            # Update gradient of discriminator.\n",
    "            optimizerD.step()\n",
    "\n",
    "            #############################\n",
    "            # (2) Compute Valid data\n",
    "            #############################\n",
    "            netD.zero_grad()\n",
    "\n",
    "            valid_data_Var = numpy_to_var(next(valid_iter)['X'], cuda)\n",
    "            D_real_valid = netD(valid_data_Var)\n",
    "            D_real_valid = D_real_valid.mean()  # avg loss\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake_valid = netG(noise_Var)\n",
    "            D_fake_valid = netD(fake_valid)\n",
    "            D_fake_valid = D_fake_valid.mean()\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty_valid = calc_gradient_penalty(netD, valid_data_Var.data,\n",
    "                                                           fake_valid.data, batch_size, lmbda,\n",
    "                                                           use_cuda=cuda)\n",
    "            # Compute metrics and record in batch history.\n",
    "            D_cost_valid = D_fake_valid - D_real_valid + gradient_penalty_valid\n",
    "            D_wass_valid = D_real_valid - D_fake_valid\n",
    "\n",
    "            if cuda:\n",
    "                D_cost_train = D_cost_train.cpu()\n",
    "                D_wass_train = D_wass_train.cpu()\n",
    "                D_cost_valid = D_cost_valid.cpu()\n",
    "                D_wass_valid = D_wass_valid.cpu()\n",
    "\n",
    "            # Record costs\n",
    "            D_cost_train_epoch.append(D_cost_train.data.numpy())\n",
    "            D_wass_train_epoch.append(D_wass_train.data.numpy())\n",
    "            D_cost_valid_epoch.append(D_cost_valid.data.numpy())\n",
    "            D_wass_valid_epoch.append(D_wass_valid.data.numpy())\n",
    "\n",
    "        #############################\n",
    "        # (3) Train Generator\n",
    "        #############################\n",
    "        # Prevent discriminator update.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Reset generator gradients\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Noise\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False)\n",
    "\n",
    "        fake = netG(noise_Var)\n",
    "        G = netD(fake)\n",
    "        G = G.mean()\n",
    "\n",
    "        # Update gradients.\n",
    "        G.backward(neg_one)\n",
    "        G_cost = -G\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Record costs\n",
    "        if cuda:\n",
    "            G_cost = G_cost.cpu()\n",
    "        G_cost_epoch.append(G_cost.data.numpy())\n",
    "\n",
    "        if i % (BATCH_NUM // 5) == 0:\n",
    "            LOGGER.info(\"{} Epoch={} Batch: {}/{} D_c:{:.4f} | D_w:{:.4f} | G:{:.4f}\".format(time_since(start), epoch,\n",
    "                                                                                             i, BATCH_NUM,\n",
    "                                                                                             D_cost_train.data.numpy(),\n",
    "                                                                                             D_wass_train.data.numpy(),\n",
    "                                                                                             G_cost.data.numpy()))\n",
    "\n",
    "    # Save the average cost of batches in every epoch.\n",
    "    D_cost_train_epoch_avg = sum(D_cost_train_epoch) / float(len(D_cost_train_epoch))\n",
    "    D_wass_train_epoch_avg = sum(D_wass_train_epoch) / float(len(D_wass_train_epoch))\n",
    "    D_cost_valid_epoch_avg = sum(D_cost_valid_epoch) / float(len(D_cost_valid_epoch))\n",
    "    D_wass_valid_epoch_avg = sum(D_wass_valid_epoch) / float(len(D_wass_valid_epoch))\n",
    "    G_cost_epoch_avg = sum(G_cost_epoch) / float(len(G_cost_epoch))\n",
    "\n",
    "    D_costs_train.append(D_cost_train_epoch_avg)\n",
    "    D_wasses_train.append(D_wass_train_epoch_avg)\n",
    "    D_costs_valid.append(D_cost_valid_epoch_avg)\n",
    "    D_wasses_valid.append(D_wass_valid_epoch_avg)\n",
    "    G_costs.append(G_cost_epoch_avg)\n",
    "\n",
    "    LOGGER.info(\"{} D_cost_train:{:.4f} | D_wass_train:{:.4f} | D_cost_valid:{:.4f} | D_wass_valid:{:.4f} | \"\n",
    "                \"G_cost:{:.4f}\".format(time_since(start),\n",
    "                                       D_cost_train_epoch_avg,\n",
    "                                       D_wass_train_epoch_avg,\n",
    "                                       D_cost_valid_epoch_avg,\n",
    "                                       D_wass_valid_epoch_avg,\n",
    "                                       G_cost_epoch_avg))\n",
    "\n",
    "    # Generate audio samples.\n",
    "    if epoch % epochs_per_sample == 0:\n",
    "        LOGGER.info(\"Generating samples...\")\n",
    "        sample_out = netG(sample_noise_Var)\n",
    "        if cuda:\n",
    "            sample_out = sample_out.cpu()\n",
    "        sample_out = sample_out.data.numpy()\n",
    "        save_samples(sample_out, epoch, output_dir)\n",
    "\n",
    "    # TODO\n",
    "    # Early stopping by Inception Score(IS)\n",
    "\n",
    "LOGGER.info('>>>>>>>Training finished !<<<<<<<')\n",
    "\n",
    "# Save model\n",
    "LOGGER.info(\"Saving models...\")\n",
    "netD_path = os.path.join(output_dir, \"discriminator.pkl\")\n",
    "netG_path = os.path.join(output_dir, \"generator.pkl\")\n",
    "torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Plot loss curve.\n",
    "LOGGER.info(\"Saving loss curve...\")\n",
    "plot_loss(D_costs_train, D_wasses_train,\n",
    "          D_costs_valid, D_wasses_valid, G_costs, output_dir)\n",
    "\n",
    "LOGGER.info(\"All finished!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
